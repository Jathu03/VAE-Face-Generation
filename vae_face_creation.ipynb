{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f9d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9baba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# define the dataset class\n",
    "class CelebADataset(Dataset):\n",
    "    # initialize the object to create the list of image paths when defining the class\n",
    "    def __init__(self,root_dir,transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir(string): Directory of the images.\n",
    "            transform: Optional transform to be applied to an image.(Data Augmentation)\n",
    "        \"\"\"\n",
    "        self.root_dir=root_dir\n",
    "        self.transform=transform\n",
    "\n",
    "        # get all image file paths from the directory and create a list\n",
    "        \"\"\"\n",
    "            1. Iterate each item in the directory in the directory using the os.listdir.\n",
    "            2. Check the items extension using .endswith('jpg') function.\n",
    "            3. If the item is an jpg image, get the path using os.join.path and add to the list.\n",
    "        \"\"\"\n",
    "        self.image_paths=[os.path.join(root_dir,img) for img in os.listdir(root_dir) if img.endswith('jpg')]  \n",
    "\n",
    "    # check the length of the class(No of images in the folder)\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # Load image using idx value as index to the image path list which is created at the initialization\n",
    "        img_path=self.image_paths[idx]\n",
    "        image=Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Apply transform if applied\n",
    "        if self.transform:\n",
    "            image=self.transform(image)\n",
    "\n",
    "        return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db4021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images loaded: 202599\n",
      "Length of the train and test datasets are (162, 202437)\n"
     ]
    }
   ],
   "source": [
    "# define the transformations we want to apply to the images\n",
    "tensor_transforms=transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the dataset from the directory\n",
    "dataset_path='img_align_celeba'\n",
    "dataset=CelebADataset(root_dir=dataset_path,transform=tensor_transforms)\n",
    "train_size = int(0.8 * len(dataset)/1000)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset,test_dataset = random_split(dataset,[train_size,test_size])\n",
    "# create Dataloader\n",
    "dataloader=DataLoader(dataset,batch_size=128,shuffle=True,num_workers=0)  # we cannot feed the entire images at once, so we feed the images as batches\n",
    "# shuffle= True ensures that everytime there is different set of images for each epoch\n",
    "# check the count of images loaded\n",
    "print(f\"Total number of images loaded: {len(dataset)}\")\n",
    "print(f'Length of the train and test datasets are {len(train_dataset),len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7ed7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 39, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8138f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37.0599])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Fake architecture creation for the understanding\n",
    "\n",
    "    1. Simple weight matrices to parametrize the layers for encoder and the distribution parameters\n",
    "    2. Calculate the output of encoder, which is the input for the mean and std layers, by multiplying the\n",
    "    input vector by the encoder weight matrix.\n",
    "    3. Get the mean and std values by multiplying the encoder layer output by the matrices corresponding to \n",
    "    the above layers\n",
    "    4. Create a normal distribution using the mean and std.\n",
    "\"\"\"\n",
    "\n",
    "input_tensor=torch.tensor([3.])\n",
    "\n",
    "w1=torch.tensor([2.],requires_grad=True) # weight matrix for encoder layer\n",
    "w_mu=torch.tensor([8.],requires_grad=True) # weight matrix for mean layer\n",
    "w_sd=torch.tensor([9.],requires_grad=True) # weight matrix for std layer\n",
    "\n",
    "enc_out=input_tensor*w1 # output of encoder layer\n",
    "mu=enc_out*w_mu # mean value\n",
    "sd=enc_out*w_sd # std value\n",
    "\n",
    "# z=torch.normal(mu,sd) avoid this as it is not differentiable\n",
    "z=mu+sd*torch.randn(1) # reparameterization trick\n",
    "z.backward() # backpropagation for all the tensoes associated with z with requires_grad=True\n",
    "w1.grad # gradient for the mean layer weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e2143-72bb-4e2f-acd4-926777e89931",
   "metadata": {},
   "source": [
    "Define the image transformations and load the train and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38bf100-cd55-4ded-bb70-3b7abb410b36",
   "metadata": {},
   "source": [
    "Define a simple variational autoencoder model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b1bd2c-46f6-41d0-b18a-4021dc920831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearVariationalAutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=3744, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (fn_mu): Linear(in_features=32, out_features=20, bias=True)\n",
       "  (fn_logvar): Linear(in_features=32, out_features=20, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=3744, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    This can be divided as shown below or \n",
    "    1. Encoder architecture\n",
    "    2. Latent space parameters(mean and log variance) architecture\n",
    "    3. Decoder architecture\n",
    "    4. Encoder forwarding function\n",
    "    5. Reparametrization function [Can be added with encoder forwarding function.]\n",
    "    6. Decoder forwarding function [To get the generated image during the inference.]\n",
    "    7. Whole model's forwading function [ Include the encoder forwarding, reparametrization and decoder forwarding.]\n",
    "\"\"\"\n",
    "class LinearVariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self,latent_dim = 20):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder network\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3*32*39,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32)\n",
    "        )\n",
    "        # mean, std layers\n",
    "        self.fn_mu=nn.Linear(32,latent_dim)\n",
    "        self.fn_logvar=nn.Linear(32,latent_dim)\n",
    "        \n",
    "        # dcoder network\n",
    "        self.decoder= nn.Sequential(\n",
    "            nn.Linear(latent_dim,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,32*39*3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # get the latent vector, mean and variance\n",
    "    def forward_enc(self,x):\n",
    "        x=self.encoder(x)\n",
    "        mu=self.fn_mu(x)\n",
    "        logvar=self.fn_logvar(x)\n",
    "\n",
    "        sigma=torch.exp(0.5*logvar)\n",
    "        noise=torch.rand_like(sigma,device=sigma.device)  # generate noise with same shape as sigma\n",
    "        \n",
    "        z = mu + sigma*noise\n",
    "        return z, mu, logvar\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size,channels,height,width = x.shape\n",
    "        x= x.flatten(1)   # tensor of shape [batch_size,num_channels * height * width]\n",
    "        z,mu,logvar = self.forward_enc(x)\n",
    "\n",
    "        dec = self.decoder(z)\n",
    "        dec = dec.reshape(batch_size,channels,height,width) \n",
    "\n",
    "        return z, mu, logvar, dec\n",
    "    \n",
    "    # to generate image from random vector\n",
    "    def forward_dec(self,z):\n",
    "       return self.decoder(z)\n",
    "\n",
    "model= LinearVariationalAutoEncoder()\n",
    "model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa5dd9-d0e4-41f0-a8b5-176585da7d64",
   "metadata": {},
   "source": [
    "Define the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba58dc4c-487f-4907-b538-3bce04466449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAELoss(x,x_hat,mean,log_var,kl_weight=1,reconstruction_weight=1):\n",
    "    # reconstruction loss\n",
    "    pixel_mse=(x-x_hat)**2\n",
    "    pixel_mse=pixel_mse.flatten(1) # size = [batch_size, num_channels * width * height]\n",
    "    # mean of total sum of the mean squared errors of pixels of a batch across batches.(divide the pixel mse by the batch_size)\n",
    "    reconstruction_loss = pixel_mse.sum(axis=-1).mean() # axis = -1 add across row(all the values in a batch); 0 add across the column(values of batches in the same coordinte)\n",
    "    #print(f\"RL {reconstruction_loss}\")\n",
    "    # reconstruction_loss = pixel_mse.mean()   # mean of the total mse of all the batches(divide the pixel mse by total pixels in all the batches.).\n",
    "\n",
    "    # KL loss\n",
    "    kl = (1+log_var - mean**2-torch.exp(log_var))  # kl values for each batch: shape = [4,2] (shape of mean, log_var)\n",
    "    kl_per_image = -0.5*torch.sum(kl,axis=-1) # total sum across each row\n",
    "    kl_loss = torch.mean(kl_per_image) # mean of the above sum\n",
    "    #print(f\"KL {kl_per_image}\")\n",
    "    return reconstruction_loss * reconstruction_weight + kl_loss * kl_weight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2f29208-0fdf-418a-a62a-ea8ecfc5ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2483.1211)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(4,1,39,32)\n",
    "x_hat=torch.randn(4,1,39,32)\n",
    "\n",
    "mean=torch.randn(4,2)\n",
    "logvar=torch.randn(4,2)\n",
    "\n",
    "a=VAELoss(x,x_hat,mean,logvar)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66902d8f-0394-4fd7-8bb3-b926314498a7",
   "metadata": {},
   "source": [
    "Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "755f245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "165b1a0a-8a8c-4be8-8b88-944f5069d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model,\n",
    "          kl_weight,\n",
    "          train_set,\n",
    "          test_set,\n",
    "          batch_size,\n",
    "          training_iterations,\n",
    "          evaluation_iterations):\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # data loaders for the train and test set\n",
    "    train_loader=DataLoader(train_set,batch_size=64,shuffle=True,num_workers=2)\n",
    "    test_loader = DataLoader(test_set,batch_size=64,shuffle=True,num_workers=2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)\n",
    "\n",
    "    # list for storing the losses\n",
    "    train_loss = []\n",
    "    evaluation_loss = []\n",
    "\n",
    "    encoded_data_per_eval = []\n",
    "    train_losses = []\n",
    "    evaluation_losses = []\n",
    "\n",
    "    pbar = tqdm(range(training_iterations))\n",
    "\n",
    "    train = True\n",
    "    step_counter = 0\n",
    "    while train:\n",
    "        for images in train_loader:\n",
    "            images=images.to(device)\n",
    "\n",
    "            # calculate the training loss and other parameters\n",
    "            encoded,mu,logvar,decoded = model(images)\n",
    "            loss = VAELoss(images,decoded,mu,logvar,kl_weight)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # backpropagation and gradient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # evaluation (optinonal)\n",
    "            if step_counter % evaluation_iterations == 0:\n",
    "                # set the model in evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                encoded_evaluations = []\n",
    "                for images in test_loader:\n",
    "                    images = images.to(device)\n",
    "\n",
    "                    # calculate loss for evaluation\n",
    "                    encoded,mu,logvar,decoded = model(images)\n",
    "                    loss = VAELoss(images,decoded, mu,logvar,kl_weight)\n",
    "\n",
    "                    evaluation_loss.append(loss.item())\n",
    "\n",
    "                # caluculate the mean losses images\n",
    "                train_loss = np.mean(train_loss)\n",
    "                evaluation_loss = np.mean(evaluation_loss)\n",
    "\n",
    "                # add the losses of each batch to the list\n",
    "                train_losses.append(train_loss)\n",
    "                evaluation_losses.append(evaluation_loss)\n",
    "\n",
    "                \n",
    "                train_loss = []\n",
    "                evaluation_loss = []\n",
    "\n",
    "                # set the model again to the training mode after the evaluation mode\n",
    "                model.train()\n",
    "\n",
    "            step_counter+=1\n",
    "            pbar.update(1)\n",
    "\n",
    "            if step_counter >= training_iterations:\n",
    "                print(\"Completed training !\")\n",
    "                train = False\n",
    "                break\n",
    "\n",
    "    print(f\"Final training loss {train_losses[-1]}\")\n",
    "    print(f\"Final evaluation loss {evaluation_losses[-1]}\")\n",
    "\n",
    "    return model, train_losses, evaluation_losses, encoded_data_per_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e95e88-228d-4226-9075-c4f43ed308c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_weight: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "kl_weights = [45]\n",
    "models = []\n",
    "encoded_datas = []\n",
    "for kl_weight in kl_weights:\n",
    "    model = LinearVariationalAutoEncoder()\n",
    "    print(f\"kl_weight: {kl_weight}\")\n",
    "    model,train_losses, evealuation_losses, encoded_data_per_eval = train(model, \n",
    "                                                                          kl_weight = kl_weight, \n",
    "                                                                          train_set = train_dataset, \n",
    "                                                                          test_set = test_dataset, \n",
    "                                                                          batch_size = 64, \n",
    "                                                                          training_iterations = 100, \n",
    "                                                                          evaluation_iterations = 20)\n",
    "    encoded_datas.append(encoded_data_per_eval)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51877a-aa1d-4a6a-ae87-25a0e25abd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ba43523160>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae=models[0]\n",
    "z = torch.randn(20)\n",
    "z=torch.tensor(z).unsqueeze(0).type(torch.float32)\n",
    "with torch.no_grad():\n",
    "    generated = vae.forward_dec(z)\n",
    "generated = generated.cpu().reshape(32,32).numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a192035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenov\\AppData\\Local\\Temp\\ipykernel_5648\\2266519356.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z=torch.tensor(z).unsqueeze(0).type(torch.float32)\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(20)\n",
    "z=torch.tensor(z).unsqueeze(0).type(torch.float32)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ee0368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 196])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "num_patches = 196\n",
    "embed_dim = 768\n",
    "\n",
    "position_embedding = nn.Embedding(num_patches,embed_dim)\n",
    "position_ids = torch.arange(num_patches).expand((4,10,-1))\n",
    "position_ids.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f77733f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 36])\n"
     ]
    }
   ],
   "source": [
    "tens = torch.randn(1,14, 36)\n",
    "print(tens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec63b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 36])\n"
     ]
    }
   ],
   "source": [
    "pat = tens[0].detach()\n",
    "print(pat.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cost_model)",
   "language": "python",
   "name": "cost_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
